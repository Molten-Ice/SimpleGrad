{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Equations from \"Neural Networks and Deep Learning\" Chapter 1\n",
    "\n",
    "## Perceptron Output Rule\n",
    "The perceptron output is determined as follows:\n",
    "$$\n",
    "\\text{output} =\n",
    "\\begin{cases} \n",
    "0 & \\text{if } \\sum_j w_j x_j \\leq \\text{threshold}, \\\\\n",
    "1 & \\text{if } \\sum_j w_j x_j > \\text{threshold}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Reformulated using the bias $b = -\\text{threshold}$:\n",
    "$$\n",
    "\\text{output} =\n",
    "\\begin{cases} \n",
    "0 & \\text{if } w \\cdot x + b \\leq 0, \\\\\n",
    "1 & \\text{if } w \\cdot x + b > 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Sigmoid Neuron Output\n",
    "The sigmoid function is defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "where $z = w \\cdot x + b$.\n",
    "\n",
    "Expanded form:\n",
    "$$\n",
    "\\text{output} = \\frac{1}{1 + \\exp(-\\sum_j w_j x_j - b)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Notation for Gradients and Updates\n",
    "For small changes in weights $\\Delta w_j$ and bias $\\Delta b$, the output change is approximated as:\n",
    "$$\n",
    "\\Delta \\text{output} \\approx \\sum_j \\frac{\\partial \\text{output}}{\\partial w_j} \\Delta w_j + \\frac{\\partial \\text{output}}{\\partial b} \\Delta b.\n",
    "$$\n",
    "\n",
    "The gradient vector $\\nabla C$ is defined as:\n",
    "$$\n",
    "\\nabla C = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial C}{\\partial w_1} \\\\\n",
    "\\frac{\\partial C}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial C}{\\partial b}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Quadratic Cost Function\n",
    "The cost function quantifies the difference between the expected and actual outputs:\n",
    "$$\n",
    "C(w, b) = \\frac{1}{2n} \\sum_x \\| y(x) - a \\|^2,\n",
    "$$\n",
    "where:\n",
    "- $w$ are the weights,\n",
    "- $b$ are the biases,\n",
    "- $n$ is the number of training inputs,\n",
    "- $y(x)$ is the expected output for input $x$,\n",
    "- $a$ is the network's actual output.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Rule\n",
    "Weights and biases are updated using the gradient of the cost function:\n",
    "$$\n",
    "w_k \\to w_k' = w_k - \\eta \\frac{\\partial C}{\\partial w_k},\n",
    "$$\n",
    "$$\n",
    "b_l \\to b_l' = b_l - \\eta \\frac{\\partial C}{\\partial b_l},\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Gradient Descent for Mini-Batches\n",
    "For a mini-batch of size $m$, update weights and biases as:\n",
    "$$\n",
    "w_k \\to w_k' = w_k - \\frac{\\eta}{m} \\sum_{j=1}^m \\frac{\\partial C_{X_j}}{\\partial w_k},\n",
    "$$\n",
    "$$\n",
    "b_l \\to b_l' = b_l - \\frac{\\eta}{m} \\sum_{j=1}^m \\frac{\\partial C_{X_j}}{\\partial b_l},\n",
    "$$\n",
    "where $X_j$ represents the $j$-th training example in the mini-batch.\n",
    "\n",
    "---\n",
    "\n",
    "## Feedforward Output Calculation\n",
    "The activation $a'$ of the next layer is computed as:\n",
    "$$\n",
    "a' = \\sigma(w a + b),\n",
    "$$\n",
    "where:\n",
    "- $a$ is the activation vector of the current layer,\n",
    "- $w$ is the weight matrix,\n",
    "- $b$ is the bias vector,\n",
    "- $\\sigma$ is applied element-wise.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Approximation\n",
    "The change in cost $C$ due to a small step $\\Delta v$ is approximated as:\n",
    "$$\n",
    "\\Delta C \\approx \\nabla C \\cdot \\Delta v.\n",
    "$$\n",
    "\n",
    "Gradient descent update rule:\n",
    "$$\n",
    "v \\to v' = v - \\eta \\nabla C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative of the Sigmoid Function\n",
    "The derivative of the sigmoid function is:\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z) (1 - \\sigma(z)).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-Batch Updates for Training\n",
    "To update weights and biases with backpropagation for a mini-batch of examples:\n",
    "1. Compute the gradients for each example $(x, y)$:\n",
    "   $$ \\delta w_k = \\frac{\\partial C_{x,y}}{\\partial w_k}, \\quad \\delta b_l = \\frac{\\partial C_{x,y}}{\\partial b_l}. $$\n",
    "2. Average the gradients across all examples in the mini-batch:\n",
    "   $$ \\nabla w_k = \\frac{1}{m} \\sum_{i=1}^m \\delta w_k^{(i)}, \\quad \\nabla b_l = \\frac{1}{m} \\sum_{i=1}^m \\delta b_l^{(i)}. $$\n",
    "3. Update weights and biases:\n",
    "   $$ w_k \\to w_k - \\eta \\nabla w_k, \\quad b_l \\to b_l - \\eta \\nabla b_l. $$\n",
    "\n",
    "---\n",
    "\n",
    "These equations cover the core mathematical tools used in the neural network and gradient descent training processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Equations from \"Neural Networks and Deep Learning\" Chapter 2\n",
    "\n",
    "## Feedforward Activation (Matrix Form)\n",
    "The activation of the \\( l \\)-th layer is computed using the following equation:\n",
    "$$\n",
    "a^l = \\sigma(w^l a^{l-1} + b^l),\n",
    "$$\n",
    "where:\n",
    "- \\( a^l \\) is the activation vector of the \\( l \\)-th layer,\n",
    "- \\( w^l \\) is the weight matrix for the \\( l \\)-th layer,\n",
    "- \\( b^l \\) is the bias vector for the \\( l \\)-th layer,\n",
    "- \\( \\sigma \\) is the activation function applied element-wise.\n",
    "\n",
    "---\n",
    "\n",
    "## Weighted Input\n",
    "The weighted input \\( z^l \\) to the \\( l \\)-th layer is defined as:\n",
    "$$\n",
    "z^l = w^l a^{l-1} + b^l.\n",
    "$$\n",
    "The activation is related to the weighted input by:\n",
    "$$\n",
    "a^l = \\sigma(z^l).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Quadratic Cost Function\n",
    "The cost function for the network is given by:\n",
    "$$\n",
    "C = \\frac{1}{2n} \\sum_x \\| y(x) - a^L(x) \\|^2,\n",
    "$$\n",
    "where:\n",
    "- \\( n \\) is the number of training examples,\n",
    "- \\( y(x) \\) is the desired output for input \\( x \\),\n",
    "- \\( a^L(x) \\) is the output of the network for input \\( x \\),\n",
    "- \\( L \\) is the index of the output layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Error in the Output Layer\n",
    "The error \\( \\delta^L \\) for the output layer is computed as:\n",
    "$$\n",
    "\\delta^L = \\nabla_a C \\odot \\sigma'(z^L),\n",
    "$$\n",
    "where:\n",
    "- \\( \\nabla_a C \\) is the gradient of the cost with respect to the activations,\n",
    "- \\( \\sigma'(z^L) \\) is the derivative of the activation function with respect to \\( z^L \\),\n",
    "- \\( \\odot \\) denotes the Hadamard (element-wise) product.\n",
    "\n",
    "For the quadratic cost function:\n",
    "$$\n",
    "\\nabla_a C = a^L - y.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Error Propagation (Backpropagation)\n",
    "The error \\( \\delta^l \\) for layer \\( l \\) is related to the error in the next layer \\( l+1 \\):\n",
    "$$\n",
    "\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "$$\n",
    "where \\( (w^{l+1})^T \\) is the transpose of the weight matrix for the \\( l+1 \\)-th layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradients for Weights and Biases\n",
    "The gradient of the cost function with respect to the weights and biases is given by:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l} = a^{l-1} (\\delta^l)^T,\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^l} = \\delta^l.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Fully Matrix-Based Backpropagation (Output Layer)\n",
    "Using a matrix \\( X = [x_1, x_2, \\dots, x_m] \\) containing the mini-batch, the backpropagation equations become:\n",
    "1. Feedforward:\n",
    "   $$ Z^l = W^l A^{l-1} + B^l, \\quad A^l = \\sigma(Z^l). $$\n",
    "2. Compute output layer error:\n",
    "   $$ \\Delta^L = \\nabla_a C \\odot \\sigma'(Z^L). $$\n",
    "3. Backpropagate errors:\n",
    "   $$ \\Delta^l = (W^{l+1})^T \\Delta^{l+1} \\odot \\sigma'(Z^l). $$\n",
    "4. Gradients for weights and biases:\n",
    "   $$ \\frac{\\partial C}{\\partial W^l} = \\Delta^l (A^{l-1})^T, \\quad \\frac{\\partial C}{\\partial B^l} = \\Delta^l. $$\n",
    "\n",
    "---\n",
    "\n",
    "These equations encapsulate the essence of backpropagation for gradient computation in neural networks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
